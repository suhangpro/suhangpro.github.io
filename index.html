<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Hang Su">
    <meta http-equiv="pragma" content="no-cache"/>
    <link rel="shortcut icon" href="img/nvidia_icon.ico">

    <title>Hang Su | NVIDIA</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/styles.css" rel="stylesheet">
    
    <!-- Custom layout styles -->
    <style>
        /* Mobile: Stack sidebar above content */
        @media (max-width: 800px) {
            #sideinfo {
                position: static !important;
                width: 100% !important;
                right: auto !important;
                top: auto !important;
                margin-bottom: 30px !important;
                text-align: center;
            }
            .main-wrapper {
                padding-right: 0 !important;
            }
        }
        
        /* Medium screens: Conservative side-by-side layout */
        @media (min-width: 801px) and (max-width: 1199px) {
            #sideinfo {
                position: fixed !important;
                top: 80px !important;
                width: 240px !important;
                z-index: 100 !important;
                right: 15px !important;
                text-align: center !important;
            }
            
            .main-wrapper {
                padding-right: 270px !important;
            }
            
            .main-container {
                max-width: 500px !important;
                margin: 0 auto !important;
            }
        }
        
        /* Large screens: Balanced layout */
        @media (min-width: 1200px) and (max-width: 1399px) {
            #sideinfo {
                position: fixed !important;
                top: 80px !important;
                width: 250px !important;
                z-index: 100 !important;
                right: max(20px, calc((100vw - 1100px) / 2)) !important;
                text-align: center !important;
            }
            
            .main-wrapper {
                padding-right: max(280px, calc((100vw - 1100px) / 2 + 260px)) !important;
            }
            
            .main-container {
                max-width: 700px !important;
                margin: 0 auto !important;
            }
        }
        
        /* Very large screens: Optimal positioning */
        @media (min-width: 1400px) {
            #sideinfo {
                position: fixed !important;
                top: 80px !important;
                width: 250px !important;
                z-index: 100 !important;
                right: calc((100vw - 800px) / 2 - 280px) !important;
                text-align: center !important;
            }
            
            .main-wrapper {
                padding-right: 0 !important;
            }
            
            .main-container {
                max-width: 800px !important;
                margin: 0 auto !important;
            }
        }
        
        /* Desktop project thumbnail responsiveness */
        @media (min-width: 801px) {
            .project-item .col-sm-4 {
                text-align: center !important;
                display: flex !important;
                align-items: flex-start !important;
                justify-content: center !important;
            }
            
            .project-thumbnail {
                display: block !important;
                width: 100% !important;
            }
            
            .project-thumbnail img {
                width: 100% !important;
                max-width: 200px !important;
                height: auto !important;
                margin: 0 auto !important;
                display: block !important;
                object-fit: contain !important;
            }
        }
        
        /* Mobile project thumbnail responsiveness */
        @media (max-width: 800px) {
            .project-item {
                display: flex !important;
                flex-direction: column !important;
                text-align: center !important;
            }
            
            .project-item .col-sm-4 {
                width: 100% !important;
                margin-bottom: 20px !important;
                padding: 0 !important;
            }
            
            .project-item .col-sm-8 {
                width: 100% !important;
                text-align: left !important;
            }
            
            .project-thumbnail img {
                width: 100% !important;
                height: auto !important;
                max-height: 50vw !important; /* Height cannot exceed 50% of viewport width - applies to all project images */
                max-width: 100% !important;
                object-fit: contain !important;
                margin: 0 auto !important;
                display: block !important;
            }
        }
        
        /* Green theme for normal text links only - exclude buttons and special elements */
        .main p a,
        .sec p a,
        .pub a,
        #sideinfo a {
            color: #2e7d32 !important;
            text-decoration: none !important;
        }
        
        .main p a:hover,
        .sec p a:hover,
        .pub a:hover,
        #sideinfo a:hover {
            color: #1b5e20 !important;
            text-decoration: underline !important;
        }
        
        .main p a:visited,
        .sec p a:visited,
        .pub a:visited,
        #sideinfo a:visited {
            color: #388e3c !important;
        }
        
        /* Exclude styled elements from green theme */
        .btnlink,
        .award-highlight a {
            color: initial !important;
        }
        
        /* White text for navigation bar */
        .navbar-inverse .navbar-nav > li > a {
            color: #ffffff !important;
        }
        
        .navbar-inverse .navbar-nav > li > a:hover,
        .navbar-inverse .navbar-nav > li > a:focus {
            color: #f0f0f0 !important;
        }
        
        .navbar-inverse .navbar-nav > .active > a,
        .navbar-inverse .navbar-nav > .active > a:hover,
        .navbar-inverse .navbar-nav > .active > a:focus {
            color: #ffffff !important;
        }
        
        .navbar-inverse .navbar-nav > .dropdown > a .caret {
            border-top-color: #ffffff !important;
            border-bottom-color: #ffffff !important;
        }
        
        .dropdown-menu > li > a {
            color: #333 !important;
        }
        
        .dropdown-menu > li > a:hover,
        .dropdown-menu > li > a:focus {
            color: #262626 !important;
            background-color: #f5f5f5 !important;
        }
    </style>

    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]>
    <script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>
<!-- top navbar-->
<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#"></a>
        </div>
        <div class="collapse navbar-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li class="active"><a href="#">Home</a></li>
                <li><a href="#">About</a></li>
                <li class="dropdown">
                    <a class="dropdown-toggle" data-toggle="dropdown">Projects<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li class="research-project"><a href="#l4p" data-target=".navbar-collapse">L4P: 4D Vision Perception</a></li>
                        <li class="research-project"><a href="#zero-msf" data-target=".navbar-collapse">Monocular Scene Flow</a></li>
                        <li class="research-project"><a href="#hybriddepth" data-target=".navbar-collapse">HybridDepth</a></li>
                        <li class="research-project"><a href="#surround-view" data-target=".navbar-collapse">Surround View Synthesis</a></li>
                        <li class="research-project"><a href="#blobgen3d" data-target=".navbar-collapse">BlobGEN-3D</a></li>
                        <li class="research-project"><a href="#fova-depth" data-target=".navbar-collapse">FoVA-Depth</a></li>
                        <li class="research-project"><a href="#nvtorchcam" data-target=".navbar-collapse">nvTorchCam</a></li>
                        <li class="research-project"><a href="#mobile-ar-depth" data-target=".navbar-collapse">Mobile AR Depth</a></li>
                        <li class="research-project"><a href="#pac" data-target=".navbar-collapse">Pixel-Adaptive Convolution</a></li>
                        <li class="research-project"><a href="#hnh" data-target=".navbar-collapse">Half&Half</a></li>
                        <li class="research-project"><a href="#splatnet" data-target=".navbar-collapse">SPLATNet</a></li>
                        <li class="research-project"><a href="#erdosrenyi" data-target=".navbar-collapse">Face Clustering</a></li>
                        <li class="research-project"><a href="#mvcnn" data-target=".navbar-collapse">MVCNN</a></li>
                        <li class="research-project"><a href="#scene-attr">Scene Attributes</a></li>
                        <li class="earlier-project"><a href="#gloc" data-target=".navbar-collapse">GLOC Occlusion</a></li>
                        <li class="earlier-project"><a href="#face-detection">Face & Body Detection</a></li>
                        <li class="earlier-project"><a href="#photo-quality">Photo Quality Assessment</a></li>
                        <li class="earlier-project"><a href="#car-detection">Vehicle Detection</a></li>
                        <li class="earlier-project"><a href="#3d-model">3D Modelling</a></li>
                    </ul>
                </li>
                <!--<li><a href="#mycalendar">Calendar</a></li>
                -->
            </ul>
        </div><!--/.nav-collapse -->
    </div>
</div>
<!-- END top navbar-->

<!-- SIDE INFO SECTION -->
<div class="col-sm-3 side" id="sideinfo" style="position: fixed; top: 80px; right: 20px; width: 250px; z-index: 100;">
            <a href="">
                <picture>
                    <source media="(min-width: 801px)" srcset="img/portrait_cat_long.jpg">
                    <source media="(max-width: 800px)" srcset="img/portrait_cat_sq.jpg">
                    <img src="img/portrait_cat_long.jpg" width="225" alt="My face" class="img-thumbnail">
                </picture>
            </a>
            <h4>Contact</h4>
            <dl>
                <dt style="font-size: 0.8em; font-weight: normal; color: #999; text-transform: uppercase; letter-spacing: 0.5px;">Office</dt>
                <dd>2 Technology Park Dr.<br>
                    NVIDIA<br>
                    Westford, MA 01886
                </dd>
                <dt style="font-size: 0.8em; font-weight: normal; color: #999; text-transform: uppercase; letter-spacing: 0.5px;">Email</dt>
                <dd><img src="img/email-crayon.png" height="16px" class="email-icon">
                </dd>
            </dl>
            <!--
             <h4>Resume</h4>
             <dl>
                 <dd><a href="https://www.dropbox.com/s/4k7gbi8ntejdryb/resume.pdf?dl=0">[pdf]</a></dd>
             </dl>
            -->
            <h4>Links</h4>
            <ul class="list-inline">
		    <!--
                <li><a href="https://www.dropbox.com/s/4k7gbi8ntejdryb/resume.pdf?dl=0">CV</a> 
                    < !--<p style="color:#DE4E2F;display:inline"> <img src="img/red-left-arrow.png" height=16px style="padding-bottom:4px;padding-left:5px"> MORE INFO!</p>
		    -- >
                    </li>
                <br/>
		    -->
                <li><a href="https://scholar.google.com/citations?user=CCFN2YwAAAAJ&hl=en"><img src="img/Google_Scholar_logo.svg" height="16px" style="margin-right: 8px;">Google Scholar</a></li>
                <br/>
                <li><a href="https://github.com/suhangpro"><img src="img/Octicons-mark-github.svg" height="16px" style="margin-right: 8px;">GitHub</a></li>
                <br/>
                <li><a href="http://www.linkedin.com/in/hangs/"><img src="img/LinkedIn_icon.svg" height="16px" style="margin-right: 8px;">LinkedIn</a></li>
            </ul>
            <h4 class="eclipse-heading">Meet Eclipse<img src="img/black-down-arrow.png" height="18px" class="arrow-icon"></h4>
            <a href="https://goo.gl/photos/SEzbqEsgrKDbyWfx7"><img src="https://www.tickerfactory.com/ezt/d/4;54;467/st/20140401/e/Eclipse+was+born/dt/0/k/b853/s-event.png"width="200" height="80" border="0" alt="Cat tickers"/></a>

            <!-- TWEETS
            <div>
                <a class="twitter-timeline" href="https://twitter.com/suhangpro" data-widget-id="439775337375735809">Tweets by @suhangpro</a>
               <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
           </div>
           TWEETS -->
</div>
<!-- END SIDE INFO SECTION -->

<div class="main-wrapper">
    <div class="container main-container" style="max-width: 800px; margin: 0 auto;">
    <!-- MAIN PANEL -->
    <div class="main" style="padding: 20px;">
            <h1 class="myanchor pagetitle page-title">Hang Su</h1>

            <!-- ABOUT SECTION -->
            <div class="sec">
                <p>
                    I'm a research scientist in the Learning and Perception Research (LPR) team at 
                    <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>. 
                    I completed my Ph.D. study in the <a href="http://vis-www.cs.umass.edu/">Computer Vision Lab</a> at 
                    UMass Amherst. Before that, I obtained my master's degree from Brown University and my bachelor's degree from Peking University. 
                </p>
                <p>
                    I work in the areas of computer vision, graphics, and deep learning. In particular, <em style="color: #2d7d32; font-weight: 600; border-bottom: 2px solid #c8e6c9; padding-bottom: 1px;">I am interested in bringing together the strengths of 2D, 3D and 4D visual perception, as well as equipping AI agents with visual information from all modalities</em>. 
                    Our work on 3D shape recognition won first place in the SHREC '16 Large-Scale 3D Shape Retrieval Contest, 
                    and I received the CVPR Best Paper Honorable Mention Award for our work on point cloud processing and the CVPR Best Paper Award Candidate for our work on monocular scene flow estimation. 
                    
                    <!--
                    I'm a 6th year PhD student in the <a href="http://vis-www.cs.umass.edu/">Computer Vision Lab</a> at
                    UMass Amherst, advised by <a href="https://people.cs.umass.edu/~elm/">Prof. Erik Learned-Miller</a>
                    and <a href="https://people.cs.umass.edu/~smaji/">Prof. Subhransu Maji</a>. I work in the areas of
                    computer vision and computer graphics, and in particular, I am interested in bringing together the
                    strengths of 2D and 3D visual information for learning richer and more flexible representations. I
                    obtained my master's degree from Brown University and my bachelor's degree from Peking University.
                    -->
                </p>
            </div>
            <!-- END ABOUT SECTION -->

            <!-- NEWS SECTION -->
            <!--
            <div class="sec">
                <h2 style="font-weight:lighter"> Recent & Upcoming</h2>
                <dl class="row">
                    <dt class="col-sm-2">12/15/23</dt>
		    <dd class="col-sm-10"><a href="https://arxiv.org/abs/2310.14437">One paper</a> accepted to HotMobile 2024. </dd>
                    <dt class="col-sm-2">10/15/23</dt>
		    <dd class="col-sm-10"><a href="https://arxiv.org/abs/2401.13786">One paper</a> accepted to 3DV 2024. </dd>
                    
                    <dt class="col-sm-2">02/24/20</dt>
                    <dd class="col-sm-10">I joined NVIDIA as a <a href="https://research.nvidia.com/person/hang-su">research scientist</a>.</dd>
                    <dt class="col-sm-2">06/16/19</dt>
                    <dd class="col-sm-10">Talk at CVPR tutorial on <a href="https://xiaolonw.github.io/graphnn/">Learning
                        Representations via Graph-structured Networks</a>.
                    </dd>
                    <dt class="col-sm-2">05/13/19</dt>
                    <dd class="col-sm-10">Our paper on <a href="#hnh">Half&Half Benchmarks</a> accepted to CVPR 2019 Workshop on Vision Meets Cognition.</dd>
                    <dt class="col-sm-2">03/11/19</dt>
                    <dd class="col-sm-10">Our paper on <a href="#pac">PACNet</a> accepted to CVPR 2019.</dd>
                    
                </dl>
            </div>
            -->
            <!-- NEWS SECTION -->

            <!-- PROJECT SECTION -->
            <div class="sec">
                <h2 class="section-heading">Research Projects</h2>
                <!--<img src="img/new.png" height=24px> between h4 and a for new pubs-->
                
                <h4 class="myanchor" id="l4p"><a href="https://research.nvidia.com/labs/lpr/l4p/">L4P: Low-level 4D Vision Perception Unified</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/snowboard_pc.gif" width="175" alt="L4P"></a></div>
                    <div class="col-sm-8">
                        <p>A unified framework for low-level 4D vision perception tasks, bridging 2D, 3D, and temporal understanding for autonomous navigation and robotics applications.</p>
                        <p>
                            <a href="https://research.nvidia.com/labs/lpr/l4p/" class="btnlink">project page</a>
                            <a href="https://arxiv.org/abs/2502.13078" class="btnlink">arXiv</a>
                            <a href="https://www.youtube.com/watch?v=uD31n0ovOig" class="btnlink">video</a>
                            <a href="https://research.nvidia.com/labs/lpr/l4p/" class="btnlink">code (coming soon)</a>
                        </p>
                        <p class="pub">Abhishek Badki*, <u>Hang Su</u>*, Bowen Wen, and Orazio Gallo,
                            &quot;<strong>L4P: Low-level 4D Vision Perception Unified</strong>&quot;, <i>arXiv 2025</i>.
                        </p>
                    </div>
                </div>
                
                <h4 class="myanchor" id="zero-msf"><a href="https://research.nvidia.com/labs/lpr/zero_msf/">Zero-shot Monocular Scene Flow Estimation in the Wild</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/zeromsf.gif" width="175" alt="Zero-shot Scene Flow"></a></div>
                    <div class="col-sm-8">
                        <p>A novel approach for zero-shot monocular scene flow estimation that works in challenging real-world scenarios without requiring training on specific datasets.</p>
                        <p class="award-highlight">üèÜ CVPR 2025 Oral, Best Paper Award Candidate</p>
                        <p>
                            <a href="https://research.nvidia.com/labs/lpr/zero_msf/" class="btnlink">project page</a>
                            <a href="https://arxiv.org/abs/2501.10357" class="btnlink">arXiv</a>
                            <a href="https://www.youtube.com/watch?v=HRcJxAOWszA" class="btnlink">video</a>
                            <a href="https://research.nvidia.com/labs/lpr/zero_msf/" class="btnlink">code (coming soon)</a>
                        </p>
                        <p class="pub">Yiqing Liang, Abhishek Badki*, <u>Hang Su</u>*, James Tompkin, and Orazio Gallo,
                            &quot;<strong>Zero-shot Monocular Scene Flow Estimation in the Wild</strong>&quot;, <i>CVPR 2025</i>.
                        </p>
                    </div>
                </div>
                
                <h4 class="myanchor" id="hybriddepth"><a href="https://github.com/cake-lab/HybridDepth">HybridDepth: Robust Depth Fusion</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/hybriddepth.png" width="175" alt="HybridDepth"></a></div>
                    <div class="col-sm-8">
                        <p>Depth estimation that combines depth from focus and single-image priors for robust metric depth fusion using focal stack images captured from a camera.</p>
                        <p>
                            <a href="https://github.com/cake-lab/HybridDepth" class="btnlink">code</a>
                            <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Ganj_HybridDepth_Robust_Metric_Depth_Fusion_by_Leveraging_Depth_from_Focus_WACV_2025_paper.pdf" class="btnlink">pdf</a>
                            <a href="https://arxiv.org/abs/2407.18443" class="btnlink">arXiv</a>
                        </p>
                        <p class="pub">Ashkan Ganj, <u>Hang Su</u>, and Tian Guo,
                            &quot;<strong>HybridDepth: Robust Metric Depth Fusion by Leveraging Depth from Focus and Single-Image Priors</strong>&quot;, <i>WACV 2025</i>.
                        </p>
                        <p class="pub">Ashkan Ganj, <u>Hang Su</u>, and Tian Guo,
                            &quot;<strong>Toward Robust Depth Fusion for Mobile AR With Depth from Focus and Single-Image Priors</strong>&quot;, <i>ISMAR 2024</i>.
                        </p>
                    </div>
                </div>
                
                <h4 class="myanchor" id="surround-view"><a href="https://youtu.be/t-UPlPlrYgQ?t=37">Surround View Synthesis for In-Car Parking Visualization</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/svs.gif" width="175" alt="Surround View Synthesis"></a></div>
                    <div class="col-sm-8">
                        <p>Comprehensive surround-view visualizations to assist with in-car parking and navigation scenarios.</p>
                        <p>
                            <a href="https://youtu.be/t-UPlPlrYgQ?t=37" class="btnlink">video</a>
                        </p>
                        <p class="pub">Abhishek Badki*, <u>Hang Su</u>*, Jan Kautz, and Orazio Gallo,
                            &quot;<strong>Surround View Synthesis for In-Car Parking Visualization</strong>&quot;, <i>NVIDIA GTC 2024</i>.
                        </p>
                    </div>
                </div>
                
                <h4 class="myanchor" id="blobgen3d"><a href="https://research.nvidia.com/labs/genair/publication/blob3dgen2024/">BlobGEN-3D: Compositional 3D-Consistent Freeview Image Generation</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/blobgen3d.png" width="175" alt="BlobGEN-3D"></a></div>
                    <div class="col-sm-8">
                        <p>A novel framework for compositional 3D-consistent freeview image generation using 3D blobs, enabling high-quality novel view synthesis with improved geometric consistency.</p>
                        <p>
                            <a href="https://research.nvidia.com/labs/genair/publication/blob3dgen2024/" class="btnlink">project page</a>
                            <a href="https://dl.acm.org/doi/10.1145/3680528.3687645" class="btnlink">paper</a>
                        </p>
                        <p class="pub">Chao Liu, Weili Nie, Sifei Liu, Abhishek Badki, <u>Hang Su</u>, Morteza Mardani, Benjamin Eckart, and Arash Vahdat,
                            &quot;<strong>BlobGEN-3D: Compositional 3D-Consistent Freeview Image Generation with 3D Blobs</strong>&quot;, <i>SIGGRAPH Asia 2024</i>.
                        </p>
                    </div>
                </div>
                
                <h4 class="myanchor" id="fova-depth"><a href="https://research.nvidia.com/labs/lpr/fova-depth/">FoVA-Depth: Field-of-View Agnostic Depth Estimation</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/fova.png" width="175" alt="FoVA-Depth"></a></div>
                    <div class="col-sm-8">
                        <p>Field-of-view agnostic depth estimation method that enables robust cross-dataset generalization, addressing the challenge of varying camera parameters at inference time and across different datasets.</p>
                        <p class="award-highlight">üèÜ 3DV 2024 Oral</p>
                        <p>
                            <a href="https://research.nvidia.com/labs/lpr/fova-depth/" class="btnlink">project page</a>
                            <a href="https://arxiv.org/abs/2401.13786" class="btnlink">arXiv</a>
                            <a href="https://github.com/NVlabs/fova-depth" class="btnlink">code</a>
                        </p>
                        <p class="pub">Daniel Lichy, <u>Hang Su</u>, Abhishek Badki, Jan Kautz, and Orazio Gallo,
                            &quot;<strong>FoVA-Depth: Field-of-View Agnostic Depth Estimation for Cross-Dataset Generalization</strong>&quot;, <i>3DV 2024</i>.
                        </p>
                    </div>
                </div>
                
                <h4 class="myanchor" id="nvtorchcam"><a href="https://github.com/NVlabs/nvTorchCam">nvTorchCam: Camera-Agnostic Differentiable Geometric Vision</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/nvtorchcam.png" width="175" alt="nvTorchCam"></a></div>
                    <div class="col-sm-8">
                        <p>An open-source PyTorch library providing camera-agnostic differentiable geometric vision operations, enabling seamless integration of various camera models in deep learning pipelines. </p>
                        <p>
                            <a href="https://research.nvidia.com/labs/lpr/fova-depth/" class="btnlink">project page</a>
                            <a href="https://arxiv.org/abs/2410.12074" class="btnlink">arXiv</a>
                            <a href="https://github.com/NVlabs/nvTorchCam" class="btnlink">code</a>
                        </p>
                        <p class="pub">Daniel Lichy, <u>Hang Su</u>, Abhishek Badki, Jan Kautz, and Orazio Gallo,
                            &quot;<strong>nvTorchCam: An Open-source Library for Camera-Agnostic Differentiable Geometric Vision</strong>&quot;, <i>arXiv 2024</i>.
                        </p>
                    </div>
                </div>
                
                <h4 class="myanchor" id="mobile-ar-depth"><a href="https://research.nvidia.com/labs/lpr/publication/ganj2024mobile/">Mobile AR Depth Estimation: Challenges & Prospects</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/depth_hotmobile.png" width="175" alt="Mobile AR Depth"></a></div>
                    <div class="col-sm-8">
                        <p>A comprehensive study examining the challenges and future prospects of depth estimation for mobile augmented reality applications, addressing key technical and practical considerations.</p>
                        <p>
                            <a href="https://arxiv.org/abs/2310.14437" class="btnlink">arXiv</a>
                        </p>
                        <p class="pub">Ashkan Ganj, Yiqin Zhao, <u>Hang Su</u>, and Tian Guo,
                            &quot;<strong>Mobile AR Depth Estimation: Challenges &amp; Prospects</strong>&quot;, <i>International Workshop on Mobile Computing Systems and Applications (HotMobile) 2024</i>.
                        </p>
                    </div>
                </div>
                
                <h4 class="myanchor" id="pac"><a href="pac/index.html">Pixel-Adaptive Convolution</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/pac.png" width="175"
                                                                     alt="PAC"></a></div>
                    <div class="col-sm-8">
                        <p>PAC is a content-adaptive operation that generalizes standard convolution and bilateral
                            filters. </p>
                        <p>
                            <a href="pac/index.html" class="btnlink">project page</a>
                            <a href="https://youtu.be/gsQZbHuR64o" class="btnlink">video</a>
                            <a href="http://arxiv.org/abs/1904.05373" class="btnlink">arXiv</a>
                            <a href="https://github.com/NVlabs/pacnet" class="btnlink">code</a>
                        </p>
                        <p class="pub"><u>Hang Su</u>, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller, and Jan Kautz,
                            &quot;<strong>Pixel-Adaptive Convolutional Neural Networks</strong>&quot;, <i>CVPR 2019</i>.
                        </p>
                    </div>
                </div>
                <h4 class="myanchor" id="hnh"><a href="#hnh">Half&Half Benchmarks</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/hnh.png" width="175"
                                                                     alt="Half&Half"></a></div>
                    <div class="col-sm-8">
                        <p>Making intelligent decisions about unseen objects given only partial observations is a fundamental component of visual common sense. In this work, we formalize prediction tasks critical to visual common sense and introduce the Half&Half benchmarks to measure an agent's ability to perform these tasks.  </p>
                        <p><a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Singh_HalfHalf_New_Tasks_and_Benchmarks_for_Studying_Visual_Common_Sense_CVPRW_2019_paper.pdf" class="btnlink">pdf</a></p>
                        <p class="pub">Ashish Singh*, <u>Hang Su</u>*, SouYoung Jin, Huaizu Jiang, Chetan Manjesh, Geng Luo, Ziwei He, Li Hong, Erik G. Learned-Miller, and Rosemary Cowell,
                            &quot;<strong>Half&Half: New Tasks and Benchmarks for Studying Visual Common Sense</strong>&quot;, <i>CVPR 2019 Workshop on Vision Meets Cognition</i>.
                        </p>
                    </div>
                </div>
                <h4 class="myanchor" id="splatnet"><a href="splatnet/index.html">SPLATNet: Sparse Lattice Networks for
                    Point Cloud Processing</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail"
                                                                                 src="img/splatnet_teaser.png"
                                                                                 width="175" alt="SPLATNET"></a></div>
                    <div class="col-sm-8">
                        <p>A fast and end-to-end trainable neural network that directly works on point clouds and can
                            also do joint 2D-3D processing.</p>
                        <p class="award-highlight">üèÜ CVPR 2018 Oral, Best Paper Award Honorable Mention</p>
                        <p class="award-highlight">üèÜ NVAIL Pioneering Research Award</p>
                        <p><a href="splatnet/index.html" class="btnlink">project page</a>
                            <a href="https://youtu.be/x18WUuBNK7E?t=5m32s" class="btnlink">video</a>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf" class="btnlink">pdf</a>
                            <a href="https://arxiv.org/abs/1802.08275" class="btnlink">arXiv</a>
                            <a href="https://github.com/NVlabs/splatnet" class="btnlink">code</a>
                        </p>

                        <p class="pub"><u>Hang Su</u>, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang,
                            and Jan Kautz, &quot;<strong>SPLATNet: Sparse Lattice Networks for Point Cloud
                                Processing</strong>&quot;, <i>CVPR 2018</i>.
                        </p>
                    </div>
                </div>
                <h4 class="myanchor" id="erdosrenyi"><a href="https://sites.google.com/site/erclustering/">End-to-end
                    Face Detection and Cast Grouping in Movies Using Erd≈ës‚ÄìR√©nyi Clustering</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail"
                                                                        src="img/hannah_teaser.png" width="175"
                                                                        alt="erdos-renyi"></a></div>
                    <div class="col-sm-8">
                        <p>An end-to-end system for detecting and clustering faces by identity in full-length
                            movies.</p>
                        <p class="award-highlight">üèÜ ICCV 2017 Spotlight</p>
                        <p>
                            <a href="https://sites.google.com/site/erclustering/" class="btnlink">project page</a>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Jin_End-To-End_Face_Detection_ICCV_2017_paper.pdf" class="btnlink">pdf</a>
                            <a href="https://arxiv.org/abs/1709.02458" class="btnlink">arXiv</a>
                            <a href="https://github.com/souyoungjin/erclustering" class="btnlink">code</a>
                        </p>
                        <p class="pub">SouYoung Jin, <u>Hang Su</u>, Chris Stauffer, and Erik Learned-Miller, &quot;<strong>End-to-end face
                            detection and cast grouping in movies using Erd≈ës‚ÄìR√©nyi clustering</strong>&quot;, <i>ICCV
                            2017</i>.
                        </p>
                    </div>
                </div>
                <h4 class="myanchor" id="mvcnn"><a href="http://vis-www.cs.umass.edu/mvcnn/">Multi-view CNN (MVCNN) for
                    3D Shape Recognition</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/mvcnn_teaser.png"
                                                                       width="175" alt="MVCNN architecture"></a></div>
                    <div class="col-sm-8">
                        <p>A novel CNN architecture that combines information from multiple views of a 3D shape into a
                            single and compact shape descriptor offering state-of-the-art performance in a range of
                            recognition tasks. </p>
                        <!-- <p class="award-highlight">üèÜ Ranked #1 in <a href="https://shapenet.cs.stanford.edu/shrec16/" class="award-link">SHREC'16 Large-Scale 3D Shape Retrieval Contest</a></p> -->
                        <p class="award-highlight">üèÜ Ranked #1 in SHREC'16 Large-Scale 3D Shape Retrieval Contest</p>
                        <p>
                            <a href="http://vis-www.cs.umass.edu/mvcnn/" class="btnlink">project page</a>
                            <a href="https://www.youtube.com/watch?v=P0ivrbPjvnM" class="btnlink">video</a>
                            <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf" class="btnlink">pdf</a>
                            <a href="https://arxiv.org/abs/1505.00880" class="btnlink">arXiv</a>
                            <a href="https://github.com/suhangpro/mvcnn" class="btnlink">code</a></p>
                        <p class="pub"><u>Hang Su</u>, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller, &quot;<strong>Multi-view
                            Convolutional Neural Networks for 3D Shape Recognition</strong>&quot;, <i>ICCV 2015</i>.
                        </p>
                        <!--
                        <p class="pub">M. Savva, F. Yu, H. Su, M. Aono, B. Chen, D. Cohen-Or, W. Deng, H. Su, S. Bai, X. Bai, N.
                            Fish, J. Han, E. Kalogerakis, E. G. Learned-Miller, Y. Li, M. Liao, S. Maji, A. Tatsuma, Y.
                            Wang, N. Zhang, and Z. Zhou, &quot;<strong>SHREC‚Äô16 Track: Large-Scale 3D Shape Retrieval
                                from ShapeNet Core55</strong>&quot;, <i>Eurographics Workshop on 3D Object Retrieval, J.
                                Jorge and M. Lin, editors, 2016</i>.
                        </p>
                        -->
                        <p class="pub">M. Savva, et al., &quot;<strong>SHREC‚Äô16 Track: Large-Scale 3D Shape Retrieval
                                from ShapeNet Core55</strong>&quot;, <i>Eurographics Workshop on 3D Object Retrieval, 2016</i>.
                        </p>
                    </div>
                </div>
                <h4 class="myanchor" id="scene-attr"><a href="#scene-attr">The SUN Attribute Database: Beyond Categories
                    for Deeper Scene Understanding</a></h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail"
                                                                            src="img/attributes.png" width="175"
                                                                            alt="Scene Attributes"></a></div>
                    <div class="col-sm-8">
                        <p>The first large-scale scene attribute database.</p>
                        <p>
                            <a href="https://cs.brown.edu/~gmpatter/sunattributes.html" class="btnlink">project page</a>
                            <a href="https://www.dropbox.com/s/bxuzy2ve5qzc0t0/SUNAttributeIJCV2014.pdf?dl=0" class="btnlink">pdf</a>
                        </p>
                        <p class="pub">G. Patterson, C. Xu, <u>H. Su</u>, J. Hays, &quot;<strong>The SUN Attribute Database: Beyond
                            Categories for Deeper Scene Understanding</strong>&quot;, <i>IJCV, May 2014</i>.
                    </div>
                </div>
            </div>
            <div class="sec">
                <h2 class="section-heading">Earlier Projects</h2>
                <h4 class="myanchor" id="gloc">Layered Global-Local (GLOC) Model for Image Parts Labelling with
                    Occlusion
                    <small>2014</small>
                </h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/gloc.png"
                                                                      width="175"
                                                                      alt="Global-Local Occlusion Model"></a></div>
                    <div class="col-sm-8">
                        <p>Learning and reasoning visual occlusions (e.g. on faces) using a deep graphical model.
                            Co-advised by <a href="http://people.cs.umass.edu/~kalo/">Professor Vangelis Kalogerakis</a>
                            and <a href="http://people.cs.umass.edu/~elm/">Professor Erik Learned-Miller.</a>
                        </p>
                        <p>We create an extension to <a href="http://vis-www.cs.umass.edu/lfw/part_labels/">LFW Part
                            Labels</a> dataset. It provides 7 part labels to 2,927 portrait photos. </p>
                        <p><a href="https://github.com/suhangpro/lfw-parts-v2" class="btnlink">data</a> (lfw-parts-v2)</p>
                    </div>

                </div>
                <h4 class="myanchor" id="face-detection">Face & Pose Detection Using Deformable Part-based Model
                    <small>2012 Summer @<a href="http://www.eharmony.com/">eHarmony</a></small>
                </h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/faces.png"
                                                                       width="175" alt="Face Detection"></a></div>
                    <div class="col-sm-8">
                        <p>In this project, I implemented in C++ a human face and body detection system based on the
                            paper "Face detection, pose estimation and landmark localization in the wild" (<i>X. Zhu and
                                D. Ramanan</i>, CVPR 2012).
                            The implementation achieves 0.95 recall and 0.90 precision on eHarmony‚Äôs user proÔ¨Åle photos.
                        </p>
                        <p><a href="https://github.com/suhangpro/dpm-face" class="btnlink">code</a></p>
                    </div>
                </div>
                <h4 class="myanchor" id="photo-quality">Photo Quality Assessment on User ProÔ¨Åle Photos
                    <small>2012</small>
                </h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/quality_smoothness.jpg" width="175"
                                                  alt="Photo Quality Assessment"></a></div>
                    <div class="col-sm-8">
                        <p>The goal of this project is to automatically distinguish high quality
                            professional photos from low quality snapshots.</p>
                        <p>We focus on assessing the quality of photos which contain faces (e.g. user profile photos).
                            We propose several image features particularly useful for this task, e.g. skin smoothness,
                            composition, bokeh. Experiments show that with small modifications they are also useful for
                            assessing other types of photos.</p>
                        <!--<p><a href="https://db.tt/SrfWg7CT">[report]</a></p>
-->
                    </div>
                </div>
                <h4 class="myanchor" id="car-detection">Front Vehicle Detection Using Onboard Camera
                    <small>2010-2011</small>
                </h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail"
                                                                               src="img/car_detection.png" width="175"
                                                                               alt="Vehicle Detection & Road Segmentation"></a>
                    </div>
                    <div class="col-sm-8">
                        <p>Onboard vehicle detection plays a key role in collision prevention and autonomous driving.
                            Camera-based detection techniques have been proven effective and economical, and show wide
                            application prospect.</p>
                        <p>This project focuses on front vehicle detection using onboard cameras. Hypothesis generation
                            based on shadows and hypothesis verification based on HOG features are combined to achieve a
                            real-time system. We also introduce and integrate a passing vehicle detection component
                            using optical flow, as well as road surface segmentation. </p>
                    </div>
                </div>
                <h4 class="myanchor" id="3d-model">3D Modelling of Peking University Campus
                    <small>2008</small>
                </h4>
                <div class="row project-item">
                    <div class="col-sm-4"><a href="#" class="project-thumbnail"><img class="img-thumbnail" src="img/sketchup.jpg"
                                                                          width="175" alt="One of the 3D models"></a>
                    </div>
                    <div class="col-sm-8">
                        <p>With almost 100 beautifully modeled 3D buildings on Peking University campus, our team won
                            the top prize in <a
                                    href="https://3dwarehouse.sketchup.com/collection/2d5e8dfbfd5d9985bdac737426f6915/Winners-Collection-2008-Google-International-Model-Your-Campus-Competition">2008
                                Google International Model Your Campus Competition</a>.</p>
                        <p>
                            <a class="btnlink" href="https://3dwarehouse.sketchup.com/collection/2d5e8dfbfd5d9985bdac737426f6915/Winners-Collection-2008-Google-International-Model-Your-Campus-Competition">project page</a>
                        </p>
                    </div>
                </div>
            </div>
            <!-- END PROJECT SECTION -->

            <!-- CALENDAR SECTION 
            <div class="sec myanchor" id="mycalendar">
                <h2 style="font-weight:lighter">My Calendar</h2>
                <div class="calendar-container">
                    <iframe src="https://www.google.com/calendar/embed?showTitle=0&amp;showNav=0&amp;showPrint=0&amp;showCalendars=0&amp;mode=WEEK&amp;height=450&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=aqmddp31t3bd93gdv2i7ve08mo%40group.calendar.google.com&amp;color=%23B1440E&amp;src=suhangpro%40gmail.com&amp;color=%23182C57&amp;ctz=America%2FNew_York"
                            style=" border-width:0 " width="800" height="450" frameborder="0" scrolling="no"></iframe>
                </div>
            </div>
            END CALENDAR SECTION -->

    </div>
    <!-- END MAIN PANEL -->
    </div>
</div>
<!-- END MAIN WRAPPER -->

<!-- bottom navbar-->
<div class="navbar navbar-default navbar-static-bottom">
    <div class="container">
        <p class="navbar-text pull-left">Powered by <a href="http://getbootstrap.com/">Bootstrap</a> | Last Updated:
            September 2025</p>
    </div>
</div>
<!-- END bottom navbar-->

<!-- Image Modal for enlarged viewing -->
<div id="imageModal" class="image-modal">
    <span class="modal-close">&times;</span>
    <img class="modal-content" id="modalImage">
    <div class="modal-caption" id="modalCaption"></div>
</div>

<!-- Bootstrap core JavaScript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="js/bootstrap.min.js"></script>

<script>
// Image modal functionality
$(document).ready(function() {
    // Open modal when thumbnail is clicked
    $('.project-thumbnail').click(function(e) {
        e.preventDefault();
        var modal = $('#imageModal');
        var modalImg = $('#modalImage');
        var caption = $('#modalCaption');
        
        modal.show();
        modalImg.attr('src', $(this).find('img').attr('src'));
        caption.text($(this).find('img').attr('alt'));
        
        // Prevent body scrolling when modal is open
        $('body').addClass('modal-open');
    });
    
    // Close modal when X is clicked or outside modal is clicked
    $('.modal-close, #imageModal').click(function(e) {
        if (e.target === this) {
            $('#imageModal').hide();
            $('body').removeClass('modal-open');
        }
    });
    
    // Prevent closing when clicking on the image itself
    $('#modalImage').click(function(e) {
        e.stopPropagation();
    });
    
    // Close modal with escape key
    $(document).keyup(function(e) {
        if (e.keyCode === 27) { // Escape key
            $('#imageModal').hide();
            $('body').removeClass('modal-open');
        }
    });
});
</script>
</body>
</html>
